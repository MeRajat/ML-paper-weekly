{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image \n",
    "Image(filename='paper.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bi-Directional Attention Flow (Bi-DAF) network, a multi stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query aware representation without early summarization. \n",
    "\n",
    "\n",
    "How attention was used in previous approaches :- \n",
    "\n",
    "    a. The computed attention weights is used to extract most relevant information from the context by summarizing the context into a fixed size vector.\n",
    "    b. Dynamic attention, attention weights at the current time step are a function of the attended vector at previous time step. \n",
    "    c. UniDirectional attention. \n",
    "    \n",
    "    \n",
    "What is different with attention here :- \n",
    "\n",
    "    a. Attention layer is not used to summarize the context graph vector. Instead attention is computed at every time step and, the attended vector at every time step, along with the representation from previous layer. \n",
    "    b. Memory less attention makes sure that, the attention at each time is a function of query and the context at current time step and does not directly depend on the previous time step \n",
    "    c. Division of work between attention layer and modelling layer \n",
    "    \n",
    "        Attention Layer :- focuses on learning the attention between query and context \n",
    "        Modelling Layer :- Focus on learning the interaction within query aware context representation(o/p of attention layer)\n",
    "    d. Bi-Directional Attention \n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional  as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('BiDAF.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is a hierarchical multi-stage process and consists of six layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1 :- Character Embedding Layer \n",
    "\n",
    "Maps each word to vector space using character level embedding <br>\n",
    "ip = N, sen_len, word_len, vocab_size <br>\n",
    "out = N, sen_len, channel_size<br>\n",
    "    \n",
    "    \n",
    "Context :- {x <sub>1</sub> ,x <sub>2</sub> , ....., x <sub>T</sub> }<br>\n",
    "Query :- {q <sub>1</sub> ,q <sub>2</sub> , ....., q <sub>J</sub> } <br>\n",
    "\n",
    "T -> Context and J -> Query <br>\n",
    "\n",
    "<b> Sequence of operations </b> <br>\n",
    "Context -> Embedding Layer -> Conv Layer -> Max Pool \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, kernel_size, out_channel):\n",
    "        super().__init__(self)\n",
    "        self.emb_size = emb_size \n",
    "        self.vocab_size = vocab_size \n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.conv = nn.Conv2d(1, out_channel, kernel_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # Shape :- (batch, seq_len, word_len, char_dim)\n",
    "        x = self.dropout(self.emb(x))\n",
    "        \n",
    "        # Make x ready for convolution layer \n",
    "        # (batch * seq_len, char_dim, word_len)\n",
    "        x = x.view(-1, emb_size, x.size(2)).unsqueeze(1)\n",
    "        \n",
    "        # (batch * seq_len, char_channel_size, 1, conv_len) \n",
    "        x  = self.conv(x)\n",
    "        # (batch, seq_len, conv_len)\n",
    "        x = x.squeeze()\n",
    "        \n",
    "        # (batch * seq_len, char_channel_size, 1)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze()\n",
    "        \n",
    "        # (batch, seq_len, char_channel_size)\n",
    "        x = x.view(batch_size, -1, self.out_channel)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 2 :- Word Embedding Layer and Highway Network \n",
    "    \n",
    "In :- (N, Sentence_len) <br>\n",
    "Out :- (N, sentence_len, embed_size) <br>\n",
    "    \n",
    "We can either make use of pretrained embeddings or train new embeddings.  Concat Character embedding and Word embedding and pass it to a highwar Network. \n",
    "\n",
    "<b> Highway Network </b> [link](https://arxiv.org/abs/1505.00387)\n",
    "\n",
    "A plain Feed forward neural network and applies a non linear transformation. \n",
    "This network consists of mainly 2 layer, normal layer(Transform gate) and gate layer(Carry gate). <br> Difference is normal layer make use of <i>relu</i> and gate layer make use of <i>sigmoid</i>\n",
    "\n",
    "In : Concat result(Context word and char embedding) X shape :-  (2d , T)\n",
    "\n",
    "    Formula :- gate * normal_layer + (1 - gate) * x  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "    \n",
    "class Highway(nn.Module):\n",
    "    def __init__(self, in_size, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.normal_layer = nn.ModuleList([nn.Linear(in_size, in_size) for _ in range(n_layers)])\n",
    "        self.gate_layer = nn.ModuleList([nn.Linear(in_size, in_size) for _ in range(n_layers)])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_layers):\n",
    "            normal_layer_ret = F.relu(self.normal_layer[i](x))\n",
    "            gate = F.sigmoid(self.gate_layer[i](x))\n",
    "            \n",
    "            x = gate * normal_layer_ret + (1 - gate) * x  \n",
    "            \n",
    "        return x \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 3 :- Contextual Embedding Layer \n",
    "\n",
    "LSTM is used on top of word embeddings which we got from previos layers to model temporal interactions between words to model temporal interactions b/w words. Bi-Directional LSTM and output is concatenated. \n",
    "\n",
    "Input H = 2d * T (d is dimension, T length of Context)\n",
    "U = 2d * J ( d is dimension, J length of query)\n",
    "\n",
    "In the first three layers, we are computing features from the query and context. Similar to the concept of convulational neural network in CV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "context_embedding_layer = nn.LSTM(input_size = hidden_size * 2, \n",
    "                                 hidden_size = hidden_size, \n",
    "                                 bidirectional = True, \n",
    "                                 batch_first = True, \n",
    "                                 dropout = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 4 :- Attentional Flow Layer \n",
    "\n",
    "It is responsible for linking and fusing information from the context and query words. Doesn't summarize instead attention vector is allowed to pass through to modelling layer. \n",
    "\n",
    "Inputs :- Comtextual vector representation of the context H and query U.\n",
    "Output :- Query aware representation of context words, G along with contextual embedding from the previous layer. \n",
    "\n",
    "We compute bidirectional attention, first form context to query and from query to context. To derive this we use Similarity matrix, which is computer by \n",
    "\n",
    "<pre>                     S = a(H<sub>:t</sub>, U<sub>:j</sub>) </pre>\n",
    "here a is trainable scalable function that encodes the similarity between two input vector H<sub>:t</sub> and U<sub>:t</sub> is t-th column vector of H and U<sub>:j</sub>, j-th column vector of U. \n",
    "\n",
    "<pre>                     a(h, u) = w<sup>T</sup><sub>(s)</sub>\\[h; u; h o u]</pre>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_emb => result of eontext embedding  (N, T, 2d)\n",
    "# query_emb => result of query embedding      (N, J, 2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape = (N, T, J, 2d)\n",
    "shape = (batch, T, J, 2 * d)\n",
    "context_emb_ex = context_emb.unsqueeze(2) #(N, T, 1, 2d)\n",
    "context_emb_ex = context_emb_ex.expand(shape) #(N, T, J, 2d)\n",
    "\n",
    "query_emb_ex = query_emb.unsqueeze(1) # (N, 1, J, 2d)\n",
    "query_emb_ex = query_emb_ex.expand(shape) #(N, T, J, 2)d\n",
    "\n",
    "a_elmwise_mul_b = torch.mul(embd_context_ex, embd_query_ex) # (N, T, J, 2d)\n",
    "concat_data = torch.cat((context_emb_ex, query_emb_ex, a_elmwise_mul_b), 3)\n",
    "\n",
    "similarity_layer = nn.Linear(6 * d, 1)\n",
    "S = similarity_layer(cat_data)\n",
    "\n",
    "S = S.view(batch_size, T. J) # (N, T, J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Similarity Matrix</b> :- Computed once and used to compute both direction of attention. S in not the attention itself it is an an unnormalized 2D similarity between the content and query words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context 2 query\n",
    "\n",
    "Shows which query words most relevant to each context word. \n",
    "\n",
    "Attention weight is computed by \n",
    "\n",
    "<pre>                    a<sub>t</sub> = softmax(S<sub>t:</sub>)</pre>\n",
    "<pre>                    w(S), shape = (6d) </pre> \n",
    "\n",
    "\\begin{equation*}\n",
    "U_{:t} = \\left( \\sum_{j} \\right)a_{tj}U_{:j}\n",
    "\\end{equation*}\n",
    "\n",
    "Here U is a 2d X T matrix, containing the attended  query vectors for entire context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context 2 query \n",
    "\n",
    "torch.bmm(F.softmax(S, dim=1), query_emb) \n",
    "\n",
    "# N, T, 2d = bmm((N, 1, T), (N, T, 2d))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2 Context :- \n",
    "<pre>\n",
    "Shows which context words have the closest similarity to one of query words and critical for answering the query. Here attention weights are obtained from \n",
    "\n",
    "                                b = Softmax(max<sub>col</sub>(S)), \n",
    "                                \n",
    "here maximum is performed over columns. So attended context vector is\n",
    "</pre>\n",
    "\n",
    "\\begin{equation*}\n",
    "H_{:t} = \\left( \\sum_{t} \\right)b_{t}H_{:t}\n",
    "\\end{equation*}\n",
    "\n",
    "Now contextual embedding and attention vectors are combined to yield G, where each column vector is considered as query aware representation of context word. \n",
    "\n",
    "\\begin{equation*}\n",
    "G = B(H_{:t}, U_{:t}, H_{:t})\n",
    "\\end{equation*}\n",
    "\n",
    "G<sub>t</sub> is the t-th column vector, corresponding to context word, B is trainable vector function, that merges 3 input vectors. <br>\n",
    "\n",
    "B is a trainable neural network, such as multilayer perceptron. Here we are using \n",
    "\n",
    "<pre>                  B(h, u<sup>~</sup>, h<sup>~</sup>) = \\[h; u; h o u; h o h\\], here B's shape is 8d.  </pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query 2 Context \n",
    "\n",
    "beta = F.softmax(torch.max(S, dim=1), query_emb) # (N, T)\n",
    "\n",
    "q2c = torch.bmm(beta.unsqueeze(1), context_emb) # (N, 1, 2d) = bmm((N, 1, T), (N, T, 2d))\n",
    "\n",
    "q2c = q2c.repeat(1, T, 1) # (N, T, 2d)  tiles T times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 5 :- Modelling Layer \n",
    "\n",
    "i/p:-  Query aware representation of context words(G).  \n",
    "o/p :- interaction among the contextual words conditioned on the query. \n",
    "\n",
    "We use two layers of bidirectional-LSTM, with output size d in each direction. Hence we obtain M with 2d shape. Which is given to output layer, where we can use it according to problem statement. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "modelling_lstm = nn.LSTM(input_size=hidden_size * 8, \n",
    "                        hidden_size = hidden_size, \n",
    "                        bidirectional = True, \n",
    "                        batch_first = True, \n",
    "                        dropout = dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 6 :- Output Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can Modify output layer according to requirements\n",
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
